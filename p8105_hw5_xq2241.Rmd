---
title: "P8105_hw5_xq2241"
author: "Xinghao Qiao"
date: 2024-11-09
output: github_document
---


# Problem 1

For a fixed group size
```{r}
# Creat a function to check if at least two people in a group share a birthday
shared_bd <- function(n) {
  bd <- sample(1:365, n, replace = TRUE)
  # Check any duplicates
  return(any(duplicated(bd)))
}

# check result
n <- 50 # assume n is 50
result <- shared_bd(n)
print(result)
```
Next,we will run this function 10000 times for each group size between 2 and 50. For each group size, we will compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs and make a plot showing the probability as a function of group size.
```{r}
set.seed(1133)
num_sim <- 10000  # # of simulations for each group size
group_sizes <- 2:50       # group sizes
prob <- numeric(length(group_sizes))  # Initialize vector to store probabilities

# Run simulations for each group size
for (i in seq_along(group_sizes)) {
  n <- group_sizes[i]
  # Run the simulation 10,000 times for each group size 
  shared_count <- sum(replicate(num_sim, shared_bd(n)))
  prob[i] <- shared_count / num_sim #  calculate probability
}

# Plot the results
plot(group_sizes, prob, col = "red",
     xlab = "Group Size (n)", ylab = "Probability of Shared Birthday",
     main = "Probability of Shared Birthday as a Function of Group Size")

```
From the plot,we can conclude that as the group size approaches 50, the probability approaches 1.And as the group size increases, the probability rises as well.


# Problem 2
First, we will you will conduct a simulation to explore power in a one-sample t-test for fixed n,sigma and mu,where n=30,sigma=5,and mu =0.
 
```{r }
library(broom)

# Set parameters for the simulation
n <- 30       # Sample size
sigma <- 5    # Sd
mu <- 0       # True mean
alpha <- 0.05 # Significance level
num <- 5000 # Number of datasets to generate

# Initialize vectors to store the results
mu_hats <- numeric(num)
p_values <- numeric(num)

# Run the simulation
set.seed(1133) 
for (i in 1:num) {
  # Generate a random sample from N(mu, sigma)
  x <- rnorm(n, mean = mu, sd = sigma)
  
  # Perform a one-sample t-test
  t_test_result <- t.test(x, mu = mu)
  
  # Extract the estimated mean and p-value using broom::tidy
  test_summary <- tidy(t_test_result)
  mu_hats[i] <- test_summary$estimate
  p_values[i] <- test_summary$p.value
  }

# Calculate the power of the test (proportion of rejections)
power <- mean(p_values < alpha)
print(power)
```

Next, we repeat the above simulation for mu from 0 to 6. And then we will make a plot showing the proportion of times the null was rejected,a plot showing the average estimate of $\hat{\mu}$ on the y axis and the true value of $\mu$ on the x axis. Make a second plot (or overlay on the first) the average estimate of $\hat{\mu}$ only in samples for which the null was rejected on the y axis and the true value of $\mu$ on the x axis.
```{r}
# Load required library
library(broom)

# Set simulation parameters
n <- 30             # Sample size
sigma <- 5          # Standard deviation
alpha <- 0.05       # Significance level
num_sim <- 5000 # Number of simulations per value of mu

# Define true mean values (effect sizes) to test, including mu = 0
mu_values <- 0:6

# Initialize vectors to store results
power_values <- numeric(length(mu_values))
avg_mu_hat <- numeric(length(mu_values))
avg_mu_hat_rejected <- numeric(length(mu_values))

# Simulation loop for each true mean 
set.seed(1133) 
for (j in seq_along(mu_values)) {
  mu <- mu_values[j]
  mu_hats <- numeric(num_sim)
  p_values <- numeric(num_sim)
  
  for (i in 1:num_sim) {
  
    x <- rnorm(n, mean = mu, sd = sigma)
    
    # Perform a one-sample t-test
    t_test_result <- t.test(x, mu = 0)
    test_summary <- tidy(t_test_result)
    
    # Store the estimated mean and p-value from the test
    mu_hats[i] <- test_summary$estimate
    p_values[i] <- test_summary$p.value
  }
  
  # Calculate power for this true mean (proportion of rejections)
  power_values[j] <- mean(p_values < alpha)
  
  # Calculate average estimate of mu hat for all samples
  avg_mu_hat[j] <- mean(mu_hats)
  
  # Calculate average estimate of mu hat only for samples where the null was rejected
  avg_mu_hat_rejected[j] <- mean(mu_hats[p_values < alpha])
}

# Plot 1: Power vs True Value of mu
plot(mu_values, power_values, type = "o", col = "blue", pch = 16,
     xlab = "True Value of mu", ylab = "Power (Proportion of Rejections)",
     main = "Power vs True Value of mu") 

# Plot 2: Average Estimate of mu hat vs True Value of mu
plot(mu_values, avg_mu_hat, type = "o", col = "blue", pch = 16,
     xlab = "True Value of mu", ylab = "Average Estimate of  hatmu",
     main = "Average Estimate of hatmu vs TrueValue of mu")

# Overlay average estimate of mu hat only for rejected samples
points(mu_values, avg_mu_hat_rejected, type = "o", col = "red", pch = 16)
legend("bottomright", legend = c("All Samples", "Rejected Samples"),
       col = c("blue", "red"), pch =16)

```
From the 2 plots, we can conclude that true value of mu increases, the power also increases.So, the larger effect sizes, the greater power we have. And in the second plot,sample average of $\hat{\mu}$ across tests for which the null is rejected approximately equal to the true value of $\mu$ when the effect size is large. However, when the effect size is small,they are not equal.


# Problem 3

```{r setup}
library(dplyr)
library(broom)
# Import raw data
url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data <- read.csv(url)
head(homicide_data)
```
The raw dataset has 52179 observations with 12 variables;uid(chr),reported_date(int),victim_last(chr),victim_first(chr),victim_race(chr),victim_age(chr),victim_sex(chr),city(chr),state(chr),lat(dbl),lon(dbl),and disposition(chr).Next, we will summarize within cities to obtain the total number of homicides and the number of unsolved homicides.
```{r}
homicide_data <- homicide_data |>
  mutate(city_state = paste(city, state, sep = ", "))

# Summarize 
summary <- homicide_data |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )


print(summary)
```
Then,we will creat a function using the prop.test function to estimate the proportion of homicides that are unsolved for the city of Baltimore, MD.
```{r}
baltimore_data <- homicide_data |>
  filter(city == "Baltimore" & state == "MD")

# total and unsolved homicides
total_homicides <- nrow(baltimore_data)
unsolved_homicides <- sum(baltimore_data$disposition %in% c("Closed without arrest", "Open/No arrest"))

# prop.test to estimate the prop 
prop_test_result <- prop.test(unsolved_homicides, total_homicides)

# Tidy the prop.test result and extract prop and ci 
tidy_result <- broom::tidy(prop_test_result)
estimated_proportion <- tidy_result$estimate
confidence_interval_lower <- tidy_result$conf.low
confidence_interval_upper <- tidy_result$conf.high

# Print the results
list(
  estimated_proportion = estimated_proportion,
  conf_low = confidence_interval_lower,
  conf_high = confidence_interval_upper
)
```
Now, we will run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each.
```{r}
library(purrr)
library(tidyr)
# city_state variable and summarize 
city_summary <- homicide_data |>
  mutate(city_state = paste(city, state, sep = ", ")) |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

# Run prop.test with continuity correction for each city and extract estimated proportions and confidence intervals
city_summary <- city_summary |>
  mutate(
    prop_test_result = map2(unsolved_homicides, total_homicides, ~ prop.test(.x, .y, correct = TRUE)), # continuity correction
    tidy_result = map(prop_test_result, tidy) # Convert prop.test result to tidy format
  ) |>
  unnest(tidy_result) |> # Unnest the tidy results
  select(city_state, estimate, conf.low, conf.high) # Select relevant columns

# rename columns
city_summary <- city_summary |>
  rename(
    estimated_proportion = estimate,
    conf_low = conf.low,
    conf_high = conf.high
  )

# tidy dataframe
print(city_summary)
```
Now,we draw the plot for this.
```{r}
library(ggplot2)
city_summary <- city_summary |>
  arrange(estimated_proportion) |>
  mutate(city_state = factor(city_state, levels = city_state))

# Draw plot with error bars
ggplot(city_summary, aes(x = city_state, y = estimated_proportion, color = estimated_proportion)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high), width = 0.2) +
  scale_color_viridis_c() +
  coord_flip() +
  labs(
    x = "City, State",
    y = "Prop. of Unsolved Homicides",
    title = "Proportion of Unsolved Homicides by City with 95% CI "
  ) +
  theme_minimal()
```

